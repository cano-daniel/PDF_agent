{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87963518",
   "metadata": {},
   "source": [
    "# Local RAG  tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0e2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ESCENARIO 2: PROCESANDO NUEVO ARCHIVO ---\n",
      "‚úÖ main_notes.pdf agregado e indexado correctamente.\n",
      "üîç An√°lisis del nuevo documento:\n",
      "\n",
      "üìç main_notes.pdf (P√°g. 2):\n",
      "-> Contents\n",
      "I Supervised learning 5\n",
      "1 Linear regression 8\n",
      "1.1 LMS algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "1.2 The normal equations . . . . . . . . . . . . . . . . . . . . . . . 13...\n",
      "\n",
      "üìç main_notes.pdf (P√°g. 214):\n",
      "-> Bibliography\n",
      "Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling\n",
      "modern machine-learning practice and the classical bias‚Äìvariance trade-\n",
      "oÓÄÉ.Proceedings of the National Academy of Sci...\n",
      "\n",
      "üìç main_notes.pdf (P√°g. 216):\n",
      "-> 215\n",
      "Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pe-\n",
      "dro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and\n",
      "rich regimes in overparametrized models.arXiv preprint a...\n"
     ]
    }
   ],
   "source": [
    "from utils.RAG import LocalRAGAgent\n",
    "\n",
    "print(\"\\n--- ESCENARIO 2: PROCESANDO NUEVO ARCHIVO ---\")\n",
    "\n",
    "# Aqu√≠ le pasamos la ruta de un archivo que NO est√° en nuestra carpeta local todav√≠a\n",
    "ruta_nuevo_pdf = \"main_notes.pdf\"\n",
    "\n",
    "# El agente lo copiar√° a local_rag/pdf_files e iniciar√° el Ryzen 7 para procesarlo\n",
    "rag_nuevo = LocalRAGAgent(pdf_path=ruta_nuevo_pdf)\n",
    "\n",
    "query_2 = \"Subjects handled in this new book?\"\n",
    "resultados_2 = rag_nuevo.search(query_2, k=3)\n",
    "\n",
    "print(f\"üîç An√°lisis del nuevo documento:\")\n",
    "for res in resultados_2:\n",
    "    print(f\"\\nüìç {res['archivo']} (P√°g. {res['pagina']}):\")\n",
    "    print(f\"-> {res['texto'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf1a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/projects/PDF_agent/agent/venv_agent/lib64/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Cargando base de datos existente con 1 archivos...\n",
      "\n",
      "[main_notes.pdf - P√°g 61]: Chapter 6\n",
      "Support vector machines\n",
      "This set of notes presents the Support Vector Machine (SVM) learning al-\n",
      "gorithm. SVMs are among the best (and many believe are indeed the best)\n",
      "ÓÄæoÓÄÉ-the-shelfÓÄäsupervi...\n",
      "\n",
      "[main_notes.pdf - P√°g 59]: nels. For instance, consider the digit recognition problem, in which given\n",
      "an image (16x16 pixels) of a handwritten digit (0-9), we have toÓÄÑgure out\n",
      "which digit it was. Using either a simple polynomia...\n",
      "--- ESCENARIO 1: CARGANDO LIBRER√çA EXISTENTE ---\n",
      "‚ö° Cargando base de datos existente con 1 archivos...\n",
      "üîç Resultados encontrados para: 'las resdes neuronales como manejan la los problermas no lienales?'\n",
      "\n",
      "[Archivo: main_notes.pdf | P√°gina: 82]\n",
      "Texto: Chapter 7\n",
      "Deep learning\n",
      "We now begin our study of deep learning. In this set of notes, we give an\n",
      "overview of neural networks, discuss vectorization and discuss training neural\n",
      "networks with backpropagation.\n",
      "7.1 Supervised learning with non-linear mo...\n",
      "\n",
      "[Archivo: main_notes.pdf | P√°gina: 84]\n",
      "Texto: 7.2 Neural networks\n",
      "Neural networks refer to broad type of non-linear models/parametrizations\n",
      "hŒ∏(x) that involve combinations of matrix multiplications and other entry-\n",
      "wise non-linear operations. We will start small and slowly build up a neural\n",
      "netw...\n"
     ]
    }
   ],
   "source": [
    "from utils.RAG import LocalRAGAgent\n",
    "# Suponiendo que ya tienes la clase LocalRAGAgent definida arriba\n",
    "print(\"--- ESCENARIO 1: CARGANDO LIBRER√çA EXISTENTE ---\")\n",
    "\n",
    "# Al no pasarle un path, el agente busca en 'local_rag/pdf_files' autom√°ticamente\n",
    "rag_existente = LocalRAGAgent() \n",
    "\n",
    "query_1 = \"las resdes neuronales como manejan la los problermas no lienales?\"\n",
    "resultados = rag_existente.search(query_1, k=2)\n",
    "\n",
    "if resultados:\n",
    "    print(f\"üîç Resultados encontrados para: '{query_1}'\")\n",
    "    for res in resultados:\n",
    "        print(f\"\\n[Archivo: {res['archivo']} | P√°gina: {res['pagina']}]\")\n",
    "        print(f\"Texto: {res['texto'][:250]}...\")\n",
    "else:\n",
    "    print(\"No se encontraron documentos en la carpeta local_rag.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b5bc85",
   "metadata": {},
   "source": [
    "# Agent class test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3564262d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/projects/PDF_agent/agent/venv_agent/lib64/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando base de datos existente...\n"
     ]
    }
   ],
   "source": [
    "from agent import dummy_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad17607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization techniques are crucial in machine learning for controlling model complexity and preventing overfitting. They work by adding an extra term, known as a regularizer, to the training loss function. This modified loss function is then minimized during training.\n",
      "\n",
      "The primary effect of regularization is to penalize complex models, effectively simplifying them. This simplification helps to reduce the model's variance, making it generalize better to unseen data. In essence, regularization strikes a balance between fitting the training data well and maintaining a simpler model structure.\n",
      "\n",
      "You can find information about regularization techniques in the following locations within the PDF:\n",
      "\n",
      "*   **Chapter 9: Regularization and model selection**: This chapter specifically discusses regularization as a technique to control model complexity and prevent overfitting. It introduces the concept of adding a regularizer term to the training loss function.\n",
      "*   **Page 103**: This page seems to be part of a section discussing generalization and regularization, likely providing further details on the topic.\n",
      "*   **Page 74**: This page is related to a Support Vector Machine (SVM) formulation, where regularization is used to control the trade-off between a large margin and the number of misclassifications.\n",
      "\n",
      "The general formula for a regularized loss function is:\n",
      "\n",
      "JŒª(Œ∏) = J(Œ∏) + ŒªR(Œ∏)\n",
      "\n",
      "Where:\n",
      "*   J(Œ∏) is the original training loss function.\n",
      "*   R(Œ∏) is the regularizer term.\n",
      "*   Œª is the regularization parameter, which controls the strength of the regularization.\n",
      "*   Œ∏ represents the model parameters.\n"
     ]
    }
   ],
   "source": [
    "dummy_friend = dummy_agent()\n",
    "mesagge = \"\"\"\n",
    "talk to me about regularization tecniques what effect do they have and were on the pdf i can find the information?\n",
    "\"\"\"\n",
    "response = dummy_friend.run_chat(mesagge)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "135fc4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a summary of the information from the specified pages and surrounding context regarding regularization techniques:\n",
      "\n",
      "**Page 74: Support Vector Machines (SVM) and Regularization**\n",
      "\n",
      "This page discusses the dual form of the Support Vector Machine (SVM) optimization problem. It explains how a regularization parameter, denoted by `C`, plays a crucial role. The parameter `C` controls the trade-off between two objectives:\n",
      "1.  Making the `w` (weight vector) norm small (which leads to a larger margin).\n",
      "2.  Ensuring that most training examples have a functional margin of at least 1.\n",
      "\n",
      "When `C` is large, the SVM tries harder to classify all points correctly, potentially leading to a more complex model and overfitting. When `C` is small, the SVM is more tolerant of misclassifications, favoring a simpler model with a larger margin, which can improve generalization.\n",
      "\n",
      "The page also mentions the Karush-Kuhn-Tucker (KKT) conditions, which relate the Lagrange multipliers (including the `Œ±i`s, which are related to `C`) to the margin and classification of the training examples.\n",
      "\n",
      "**Page 103: Generalization and Model Performance**\n",
      "\n",
      "This page introduces the fundamental concept of generalization in machine learning. It highlights the difference between:\n",
      "*   **Training loss/error**: How well a model fits the data it was trained on.\n",
      "*   **Test loss/error**: How well a model performs on unseen data.\n",
      "\n",
      "The ultimate goal of machine learning is to minimize the test error, not just the training error. Minimizing training error alone can lead to overfitting, where the model learns the training data too well, including its noise and idiosyncrasies, and thus performs poorly on new data. Generalization refers to a model's ability to perform well on unseen data.\n",
      "\n",
      "**Page 126: Regularization and Model Complexity**\n",
      "\n",
      "This page provides a more general overview of regularization. It reiterates that overfitting occurs when models are too complex. Regularization is presented as a technique to control this complexity.\n",
      "\n",
      "The core idea of regularization is to add a penalty term (the regularizer, `R(Œ∏)`) to the original loss function (`J(Œ∏)`):\n",
      "\n",
      "`JŒª(Œ∏) = J(Œ∏) + ŒªR(Œ∏)`\n",
      "\n",
      "*   `Œª` is the **regularization parameter**, which balances the importance of fitting the data (`J(Œ∏)`) versus keeping the model simple (`R(Œ∏)`).\n",
      "*   A larger `Œª` imposes a stronger penalty on complexity, leading to simpler models but potentially higher bias.\n",
      "*   A smaller `Œª` allows for more complex models, potentially reducing bias but increasing variance.\n",
      "\n",
      "The page also specifically mentions:\n",
      "\n",
      "*   **L2 regularization**: `R(Œ∏) = 1/2 ||Œ∏||¬≤‚ÇÇ`. This encourages the model parameters (weights) to be small. In deep learning, it's often called **weight decay** because gradient descent updates effectively shrink the weights.\n",
      "*   **L1 regularization**: `R(Œ∏) = ||Œ∏||‚ÇÅ`. This encourages **sparsity**, meaning many of the model parameters become zero. This can be useful when you believe only a subset of features is important. The page notes that L1 regularization is not a continuous function and thus cannot be optimized directly with gradient descent, but L1 norm (`||Œ∏||‚ÇÅ`) is often used as a continuous surrogate.\n",
      "\n",
      "**Surrounding Pages (Implicit Importance)**\n",
      "\n",
      "While not explicitly requested, understanding the context around these pages is helpful:\n",
      "\n",
      "*   **Chapters related to Model Selection and Generalization (like Chapters 8 and 9)** are crucial. They delve into the theoretical underpinnings of why models overfit and how techniques like regularization help combat this. They will likely discuss the bias-variance trade-off in detail, which is central to understanding the effects of regularization.\n",
      "*   **Pages discussing optimization algorithms (like the SMO algorithm mentioned on page 74)** are relevant because regularization changes the optimization landscape. The algorithms used to minimize the regularized loss function must be able to handle these modified objective functions.\n",
      "*   **Discussions on different types of models (e.g., linear models, neural networks)** within these chapters will illustrate how regularization is applied in various contexts.\n",
      "\n",
      "In summary, regularization techniques are essential for building machine learning models that generalize well. They work by penalizing model complexity, and their effects can be tuned using regularization parameters, as seen in the context of SVMs and general loss function minimization. The PDF covers these concepts in detail across several chapters and specific pages.\n"
     ]
    }
   ],
   "source": [
    "message = \"could you go to the pages you mentiuoned and tell me about them making a sumary or what do they talk about ALSO SEARCH IN  THE SURRONDING PAGES YOU THINK IT WOULD BE IMPORTANT\"\n",
    "response = dummy_friend.run_chat(message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac73d76",
   "metadata": {},
   "source": [
    "# Test api 'endpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ee85c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Docker:\n",
      "{'mesagge': 'La informaci√≥n sobre la interpretaci√≥n de la distancia se encuentra principalmente en la p√°gina 62 del archivo `main_notes.pdf`. Tambi√©n hay informaci√≥n relacionada en las p√°ginas 64 y 137 del mismo archivo.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The URL points to localhost because we mapped the ports in Docker\n",
    "URL = \"http://localhost:8000/search\"\n",
    "\n",
    "payload = {\n",
    "    \"text\": \"en que paginas puedo encontrar informaciopn de esto, estoy interesado en la interpreatcion de la sdistancia\",\n",
    "    \"User_id\": 'DANIEL'\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(URL, json=payload)\n",
    "    response.raise_for_status() # Check for errors\n",
    "    \n",
    "    data = response.json()\n",
    "    print(\"Response from Docker:\")\n",
    "    print(data)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c67e22fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Docker:\n",
      "{'mesagge': 'No encontr√© informaci√≥n sobre las ecuaciones de campo de Einstein en los documentos proporcionados.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Inside another container on the same network:\n",
    "URL = \"http://localhost:8000/search\"\n",
    "text = \"quiero saber sobre las ecuaciones de campo de einstein\"\n",
    "payload = {\n",
    "    \"text\": text,\n",
    "    \"User_id\": 'DANIEL'\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(URL, json=payload)\n",
    "    response.raise_for_status() # Check for errors\n",
    "    \n",
    "    data = response.json()\n",
    "    print(\"Response from Docker:\")\n",
    "    print(data)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
